{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a78977-36cf-4a86-8996-9170ca15deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes-cuda115\n",
      "  Downloading bitsandbytes_cuda115-0.26.0.post2-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes-cuda115\n",
      "Successfully installed bitsandbytes-cuda115-0.26.0.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9085ba-c47d-4f40-9164-83603ce233df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (0.37.0)\n",
      "Requirement already satisfied: datasets in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (2.10.1)\n",
      "Requirement already satisfied: accelerate in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: loralib in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (0.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: multiprocess in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (0.13.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: xxhash in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: packaging in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: pandas in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: responses<0.19 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: aiohttp in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from datasets) (3.8.0)\n",
      "Requirement already satisfied: psutil in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (58.5.3)\n",
      "Requirement already satisfied: wheel in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.37.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git@main\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-6h9y5n_w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-6h9y5n_w\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit b90fbc7e0ba41dfd6b343e7e2274443f19087f36\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-to1r15ai\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-to1r15ai\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 50aaf99da7abc04994f4478123666240aa5c0dc6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (21.0)\n",
      "Requirement already satisfied: requests in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (2022.4.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (0.13.1)\n",
      "Requirement already satisfied: filelock in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from transformers==4.27.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: psutil in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from peft==0.3.0.dev0) (5.8.0)\n",
      "Requirement already satisfied: accelerate in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from peft==0.3.0.dev0) (0.17.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from peft==0.3.0.dev0) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.27.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0.dev0) (58.5.3)\n",
      "Requirement already satisfied: wheel in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0.dev0) (0.37.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from importlib-metadata->transformers==4.27.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests->transformers==4.27.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests->transformers==4.27.0.dev0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests->transformers==4.27.0.dev0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /apps/data/wirawan/miniconda3/envs/transformers-cuda11/lib/python3.7/site-packages (from requests->transformers==4.27.0.dev0) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes datasets accelerate loralib\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a516684d-89ae-413f-8ad4-5169f6ee314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f723b7-3b02-4135-9fc0-713e34023c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloomz-1b1\"\n",
    "# model_name = \"facebook/opt-6.7b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    #load_in_8bit=True, \n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce52118-87ed-4c2f-8493-59cd1a1b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86392808-0d7b-446d-8fc3-5abf52c4e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa1f245-cda5-4250-9249-ef40ed476e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 1067673600 || trainable%: 0.22097539922313336\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    #target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedcb2e8-fd0a-41ed-ab96-2b375a8d13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.14it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a63daca5e848e5b9.arrow\n",
      "/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 19/500 00:12 < 05:50, 1.37 it/s, Epoch 0.11/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.516900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      8\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/transformers/trainer.py:1631\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1628\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1630\u001b[0m )\n\u001b[0;32m-> 1631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/transformers/trainer.py:1898\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1896\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1901\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1904\u001b[0m ):\n\u001b[1;32m   1905\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1906\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/transformers/trainer.py:2651\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2651\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sysadmin/wirawan/miniconda3/envs/lora/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100, \n",
    "        max_steps=500, \n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53ba8c-0625-454d-aed3-7714b959d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b385e9-83b6-429b-9a42-fd22349d721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"cahya/bloomz-7b1-lora\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9555b6-07c5-4786-b32f-a7f693c54103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"cahya/bloomz-7b1-lora\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140e89c-471c-46a2-8bb6-46fa991ae04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\"How old is the universe?\", return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735664f-e04d-4619-8b69-3a9dfb6a984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08567d2b-bd4e-4657-8070-54860ba7772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-1b1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7ffe8-2509-466a-9f71-1b3810aa970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\"Two things are infinite: \", return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac86a2f-693d-4b4b-9d8d-99cd72bf57cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
